{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto: Defunciones Fetales / Nacimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laura Tamath\t19365 |     Andrea Amaya 19357 |\n",
    "Brandon Hern√°ndez\t 19376 |\t\tMartin Amado\t19020 |   Juan Pablo Pineda 19087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import pyclustertend \n",
    "import random\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, confusion_matrix\n",
    "import sklearn.preprocessing\n",
    "from sklearn.cluster import Birch, KMeans\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['dep_reg', 'mun_reg', 'mon_reg', 'year_reg', 'dep_occu','mun_occu','area_geog','sex_death','day_occu',\n",
    "'month_occu','year_occu','part_type','birth_class','via_part','weeks_ges', 'mother_age' ,'mom_country_res','mom_dep_res',\n",
    "'mom_mun_resi', 'mom_group', 'mom_civil_status', 'mom_nationality', 'mom_scholarship', 'mom_occupation', 'cause_death',\n",
    "'assistance_received', 'site_occu', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "\n",
    "quan_vars = ['weeks_ges', 'mother_age', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "qual_vars = []\n",
    "for var in var_names: \n",
    "  if var not in quan_vars: qual_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(data_frame, to_remove, is_quali=True):\n",
    "  df = data_frame.copy()\n",
    "  all_vars = var_names[:]\n",
    "  remove_vars = to_remove[:]\n",
    "\n",
    "  for var in remove_vars: all_vars.remove(var)\n",
    "  df.columns = all_vars\n",
    "  for var in remove_vars:\n",
    "    df[var] = np.full(len(df.index), np.nan if is_quali else 0)\n",
    "  return df.reindex(sorted(df.columns), axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2009 = pd.read_spss('./data/db_2009.sav')\n",
    "df_2010 = pd.read_spss('./data/db_2010.sav')\n",
    "df_2011 = pd.read_spss('./data/db_2011.sav')\n",
    "df_2012 = pd.read_spss('./data/db_2012.sav')\n",
    "df_2013 = pd.read_spss('./data/db_2013.sav')\n",
    "df_2014 = pd.read_spss('./data/db_2014.sav')\n",
    "df_2015 = pd.read_spss('./data/db_2015.sav')\n",
    "df_2016 = pd.read_spss('./data/db_2016.sav')\n",
    "df_2017 = pd.read_spss('./data/db_2017.sav')\n",
    "df_2018 = pd.read_spss('./data/db_2018.sav')\n",
    "df_2019 = pd.read_spss('./data/db_2019.sav')\n",
    "df_2020 = pd.read_spss('./data/db_2020.sav')\n",
    "\n",
    "# Filter data\n",
    "remove_2009 = ['via_part', 'mom_country_res', 'mom_scholarship']\n",
    "remove_2010_2011 = ['mom_country_res']\n",
    "remove_2012_2013_2014 = ['year_occu']\n",
    "remove_2018_2019_2020 = ['area_geog']\n",
    "\n",
    "df_2009 = filter_df(df_2009, remove_2009)\n",
    "df_2009['year_occu'] = np.full(len(df_2009.index), '2009')\n",
    "df_2009['year_reg'] = np.full(len(df_2009.index), '2009')\n",
    "df_2010 = filter_df(df_2010, remove_2010_2011)\n",
    "df_2011 = filter_df(df_2011, remove_2010_2011)\n",
    "df_2012 = filter_df(df_2012, remove_2012_2013_2014)\n",
    "df_2012['year_occu'] = np.full(len(df_2012.index), '2012')\n",
    "df_2013 = filter_df(df_2013, remove_2012_2013_2014)\n",
    "df_2013['year_occu'] = np.full(len(df_2013.index), '2013')\n",
    "df_2014 = filter_df(df_2014, remove_2012_2013_2014)\n",
    "df_2014['year_occu'] = np.full(len(df_2014.index), '2014')\n",
    "df_2015= filter_df(df_2015, [])\n",
    "df_2016= filter_df(df_2016, [])\n",
    "df_2017= filter_df(df_2017, [])\n",
    "df_2018 = filter_df(df_2018, remove_2018_2019_2020)\n",
    "df_2019 = filter_df(df_2019, remove_2018_2019_2020)\n",
    "df_2020 = filter_df(df_2020, remove_2018_2019_2020)\n",
    "data = pd.concat([df_2009, df_2010, df_2011, df_2012, df_2013, df_2014, df_2015, df_2016, df_2017, df_2018, df_2019, df_2020])\n",
    "\n",
    "data['day_occu'] = data['day_occu'].astype(float)\n",
    "data['day_occu'] = data['day_occu'].astype(int)\n",
    "data['year_reg'] = data['year_reg'].astype(float)\n",
    "data['year_reg'] = data['year_reg'].astype(int)\n",
    "data['year_occu'] = data['year_occu'].astype(int)\n",
    "data[qual_vars] = data[qual_vars].astype(str)\n",
    "data = data.reset_index()\n",
    "\n",
    "\n",
    "quan_df = data[quan_vars].replace('Ignorado', -1).fillna(-1)\n",
    "data = pd.concat([quan_df, data[qual_vars]], axis=1)\n",
    "\n",
    "data = data.drop(data[data[quan_vars[0]] < 0].index)\n",
    "data = data.drop(data[data[quan_vars[1]] < 0].index)\n",
    "\n",
    "data = data.sample(n=380, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in qual_vars:\n",
    "    data[var].value_counts().plot(kind='bar')\n",
    "    plt.figure(figsize=(20,5))\n",
    "    print('\\n'+ var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of place of residence and fetal deaths\n",
    "pd.crosstab(index=data['mom_dep_res'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of residence and fetal deaths\n",
    "pd.crosstab(index=data['mom_dep_res'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of place of birth and fetal deaths\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of birth and fetal deaths\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of type of birth and fetal deaths\n",
    "pd.crosstab(index=data['part_type'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of type of birth and fetal deaths\n",
    "pd.crosstab(index=data['part_type'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mother's marital status and fetal deaths\n",
    "pd.crosstab(index=data['mom_civil_status'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of mother's marital status and fetal deaths\n",
    "pd.crosstab(index=data['mom_civil_status'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of gestation weeks and fetal deaths\n",
    "pd.crosstab(index=data['weeks_ges'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph  of gestation weeks and fetal deaths\n",
    "pd.crosstab(index=data['weeks_ges'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of place of birth and total born\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_children'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean of quant vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_df = data[quan_vars]\n",
    "quan_df[quan_vars] = quan_df[quan_vars].astype(float)\n",
    "quan_df[quan_vars] = quan_df[quan_vars].astype(int)\n",
    "\n",
    "\n",
    "graph_labels = [quan_vars[0], quan_vars[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quan_vars:\n",
    "  serie = quan_df[quan_df[var] > 0][var]\n",
    "  display(serie.describe())\n",
    "  sns.displot(quan_df[var], kde=True)\n",
    "  print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(serie), 'Skewness:', stats.skew(serie), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = quan_df.corr()\n",
    "cm = np.corrcoef(corrmat.values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=quan_vars, xticklabels=quan_vars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(quan_df, height= 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NACIMIENTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_var_names = ['dep_reg', 'mun_reg', 'mon_reg', 'year_reg', 'inscirption', 'dep_occu','mun_occu', 'area_geog', 'pounds_weight', 'ounces_weight',\n",
    "'day_occu', 'month_occu','year_occu', 'genre', 'part_type','via_part', 'dad_age', 'dad_contry', 'dep_dad', 'mun_dad',\n",
    "'dad_group', 'dad_civil', 'birth_country_dad', 'birt_dep_dad', 'birth_mun_dad', 'dad_nan', 'dad_scholar', 'dad_occup', 'mother_age', \n",
    "'mom_country_res', 'mom_dep_res', 'mom_mun_resi', 'mom_group', 'mom_civil_status', 'birth_country_mom', 'birth_dep_mom', 'birth_mun_mom',\n",
    "'mom_nationality', 'mom_scholarship', 'mom_occupation', 'assistance_received', 'site_occu', 'total_children', 'total_dead_children', \n",
    "'total_living_children']\n",
    "\n",
    "filter = ['dep_reg', 'mun_reg', 'mon_reg', 'year_reg', 'dep_occu', 'mun_occu', 'day_occu', 'month_occu', 'part_type', 'via_part',\n",
    "'mother_age', 'mom_country_res', 'mom_dep_res', 'mom_mun_resi', 'mom_group', 'mom_civil_status', 'mom_scholarship', 'mom_occupation', \n",
    "'assistance_received', 'site_occu', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "\n",
    "nc_quan_vars = ['mother_age', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "nc_qual_vars = []\n",
    "for var in filter: \n",
    "  if var not in nc_quan_vars: nc_qual_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_filter_df(data_frame, to_remove, is_quali=True):\n",
    "  df = data_frame.copy()\n",
    "  all_vars = nc_var_names[:]\n",
    "  remove_vars = to_remove[:]\n",
    "\n",
    "  for var in remove_vars: all_vars.remove(var)\n",
    "  df.columns = all_vars\n",
    "  for var in remove_vars:\n",
    "    df[var] = np.full(len(df.index), np.nan if is_quali else 0)\n",
    "  return df.reindex(sorted(df.columns), axis=1).copy()\n",
    "\n",
    "def drop_df_colums(data_frame, to_remove):\n",
    "  df = data_frame.copy()\n",
    "  all_vars = nc_var_names[:]\n",
    "  remove_vars = to_remove[:]\n",
    "\n",
    "  for var in remove_vars: all_vars.remove(var)\n",
    "  for var in all_vars:\n",
    "    df = df.drop(var, axis=1)\n",
    "  return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_2009 = pd.read_spss('./data/nc_2009.sav')\n",
    "nc_2010 = pd.read_spss('./data/nc_2010.sav')\n",
    "nc_2011 = pd.read_spss('./data/nc_2011.sav')\n",
    "nc_2012 = pd.read_spss('./data/nc_2012.sav')\n",
    "nc_2013 = pd.read_spss('./data/nc_2013.sav')\n",
    "nc_2014 = pd.read_spss('./data/nc_2014.sav')\n",
    "nc_2015 = pd.read_spss('./data/nc_2015.sav')\n",
    "nc_2016 = pd.read_spss('./data/nc_2016.sav')\n",
    "nc_2017 = pd.read_spss('./data/nc_2017.sav')\n",
    "nc_2018 = pd.read_spss('./data/nc_2018.sav')\n",
    "nc_2019 = pd.read_spss('./data/nc_2019.sav')\n",
    "nc_2020 = pd.read_spss('./data/nc_2020.sav')\n",
    "\n",
    "nc_2009 = nc_filter_df(nc_2009, ['inscirption', 'via_part', 'dad_contry', 'birth_country_dad', 'dad_scholar', 'mom_country_res', 'birth_country_mom', 'mom_scholarship'])\n",
    "nc_2009['year_reg'] = np.full(len(nc_2009.index), '2009')\n",
    "nc_2010 = nc_filter_df(nc_2010, ['inscirption', 'via_part', 'dad_contry', 'birth_country_dad', 'mom_country_res', 'birth_country_mom'])\n",
    "nc_2010['year_reg'] = np.full(len(nc_2010.index), '2009')\n",
    "nc_2011 = nc_filter_df(nc_2011, ['inscirption', 'via_part', 'dad_contry', 'birth_country_dad', 'mom_country_res', 'birth_country_mom'])\n",
    "nc_2011['year_reg'] = np.full(len(nc_2011.index), '2009')\n",
    "nc_2012 = nc_filter_df(nc_2012, ['inscirption', 'area_geog', 'year_occu', 'via_part'])\n",
    "nc_2012['year_reg'] = np.full(len(nc_2012.index), '2009')\n",
    "nc_2013 = nc_filter_df(nc_2013, ['inscirption', 'area_geog', 'year_occu', 'via_part'])\n",
    "nc_2013['year_reg'] = np.full(len(nc_2013.index), '2009')\n",
    "nc_2014 = nc_filter_df(nc_2014, ['inscirption', 'area_geog',  'year_occu', 'via_part', 'dad_nan', 'mom_nationality'])\n",
    "nc_2014['year_reg'] = np.full(len(nc_2014.index), '2009')\n",
    "nc_2015 = nc_filter_df(nc_2015, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2015['year_reg'] = np.full(len(nc_2015.index), '2009')\n",
    "nc_2016 = nc_filter_df(nc_2016, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2016['year_reg'] = np.full(len(nc_2016.index), '2009')\n",
    "nc_2017 = nc_filter_df(nc_2017, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2017['year_reg'] = np.full(len(nc_2017.index), '2009')\n",
    "nc_2018 = nc_filter_df(nc_2018, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2018['year_reg'] = np.full(len(nc_2018.index), '2009')\n",
    "nc_2019 = nc_filter_df(nc_2019, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2019['year_reg'] = np.full(len(nc_2019.index), '2009')\n",
    "nc_2020 = nc_filter_df(nc_2020, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2020['year_reg'] = np.full(len(nc_2020.index), '2009')\n",
    "\n",
    "nc_2009 = drop_df_colums(nc_2009, filter)\n",
    "nc_2010 = drop_df_colums(nc_2010, filter)\n",
    "nc_2011 = drop_df_colums(nc_2011, filter)\n",
    "nc_2012 = drop_df_colums(nc_2012, filter)\n",
    "nc_2013 = drop_df_colums(nc_2013, filter)\n",
    "nc_2014 = drop_df_colums(nc_2014, filter)\n",
    "nc_2015 = drop_df_colums(nc_2015, filter)\n",
    "nc_2016 = drop_df_colums(nc_2016, filter)\n",
    "nc_2017 = drop_df_colums(nc_2017, filter)\n",
    "nc_2018 = drop_df_colums(nc_2018, filter)\n",
    "nc_2019 = drop_df_colums(nc_2019, filter)\n",
    "nc_2020 = drop_df_colums(nc_2020, filter)\n",
    "\n",
    "nc_data = pd.concat([nc_2009, nc_2010, nc_2011, nc_2012, nc_2013, nc_2014, nc_2015, nc_2016, nc_2017, nc_2018, nc_2019, nc_2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_quan_df = nc_data[nc_quan_vars]\n",
    "nc_quan_df = nc_quan_df[nc_quan_vars].replace('Ignorado', -1).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in nc_quan_vars:\n",
    "  serie = quan_df[quan_df[var] > 0][var]\n",
    "  display(serie.describe())\n",
    "  sns.displot(quan_df[var], kde=True)\n",
    "  print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(serie), 'Skewness:', stats.skew(serie), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = nc_quan_df.corr()\n",
    "cm = np.corrcoef(corrmat.values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=quan_vars, xticklabels=quan_vars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(quan_df, height= 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in nc_qual_vars:\n",
    "    data[var].value_counts().plot(kind='bar')\n",
    "    plt.figure(figsize=(20,5))\n",
    "    print('\\n'+ var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of registration department and births\n",
    "pd.crosstab(index=data['dep_reg'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of registration township  and births\n",
    "pd.crosstab(index=data['mun_reg'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of registration month and births\n",
    "pd.crosstab(index=data['mon_reg'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of registration year and births\n",
    "pd.crosstab(index=data['year_reg'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of occurrence department and births\n",
    "pd.crosstab(index=data['dep_occu'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of occurrence township and births\n",
    "pd.crosstab(index=data['mun_occu'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of occurrence day and births\n",
    "pd.crosstab(index=data['day_occu'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of occurrence month and births\n",
    "pd.crosstab(index=data['month_occu'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of type of birth and births\n",
    "pd.crosstab(index=data['part_type'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of via of birth and births\n",
    "pd.crosstab(index=data['via_part'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom country residence and births\n",
    "pd.crosstab(index=data['mom_country_res'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom departament residence and births\n",
    "pd.crosstab(index=data['mom_dep_res'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom township residence and births\n",
    "pd.crosstab(index=data['mom_mun_resi'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom group and births\n",
    "pd.crosstab(index=data['mom_group'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom civil status and births\n",
    "pd.crosstab(index=data['mom_civil_status'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom scholarship and births\n",
    "pd.crosstab(index=data['mom_scholarship'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom occupation and births\n",
    "pd.crosstab(index=data['mom_occupation'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of asistence received and births\n",
    "pd.crosstab(index=data['assistance_received'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of site of occurrence and births\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploraci√≥n de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comience  describiendo  cuantas  variables  y  observaciones  tiene  disponibles,  el tipo de cada una de las variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Haga un resumen de las variables num√©ricas e investigue si siguen una distribuci√≥n normal y tablas de frecuencia para las variables categ√≥ricas, escriba lo que vaya encontrando. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cruce  las  variables  que  considere  que  son  las  m√°s  importantes  para  hallar  los elementos  clave  que  lo  pueden  llevar  a  comprender  lo  que  est√°  causando  el problema encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Haga gr√°ficos exploratorios que le de ideas del estado de los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Haga un agrupamiento (clustering) e interprete los resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "X_scale=sklearn.preprocessing.scale(quan_df)\n",
    "\n",
    "pyclustertend.hopkins(X_scale, len(X_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de hopkins es de 0.11, por lo que vale la pena hacer el agrupamiento al tener datos distribuidos de manera uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyclustertend.vat(X_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la gr√°fica de codo para encontrar la cantidad √≥ptima de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeroClusters = range(1,11)\n",
    "\n",
    "wcss = []\n",
    "# Obtenemos 10 posibles clusters\n",
    "for i in numeroClusters:\n",
    "    # Se calcula la kmean con esa cantidad de clusters\n",
    "    kmeans = cluster.KMeans(n_clusters=i)\n",
    "    kmeans.fit(X_scale)\n",
    "    # Obtenemos la inercia\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Graficando\n",
    "plt.plot(numeroClusters, wcss)\n",
    "plt.xlabel(\"Cantidad de clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.title(\"Gr√°fico de Codo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se har√°n uso de 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = 4\n",
    "colors = ['mediumslateblue', 'skyblue', 'pink', 'cornflowerblue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "birch_model = Birch(threshold=1.5, n_clusters=number_clusters)\n",
    "birch_model.fit(X_scale)\n",
    "\n",
    "# Obtenemos los puntos y los clusters\n",
    "birch_result = birch_model.predict(X_scale)\n",
    "\n",
    "for i in range(number_clusters):\n",
    "    # Graficar los clusters\n",
    "    plt.scatter(X_scale[birch_result == i, 0], X_scale[birch_result == i, 1], s = 100, c = colors[i], label = \"Cluster %d\" %i)\n",
    "\n",
    "plt.title(\"Metodo de BIRCH\")\n",
    "plt.xlabel(graph_labels[0])\n",
    "plt.ylabel(graph_labels[1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_silhouette(clusterer, n_clusters, label):\n",
    "    fig, ax = plt.subplots(figsize=(1,1))\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1])\n",
    "    ax.set_ylim([0, len(X_scale) + (n_clusters + 1) * 10])\n",
    "\n",
    "    cluster_labels = clusterer.fit_predict(X_scale)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X_scale, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score of\",\n",
    "        label,\n",
    "        'is:',\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    sample_silhouette_values = silhouette_samples(quan_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = colors[i]\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([]) \n",
    "    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "make_silhouette(birch_model, number_clusters, 'BIRCH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps = ['dep_reg', 'assistance_received', 'site_occu', 'cause_death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiando las variables cualitativas a numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stamp in stamps:\n",
    "  aux = data.groupby(by=stamp)\n",
    "  \n",
    "  \n",
    "  tag_group = list(aux.groups.keys())\n",
    "  tag_group = [x.lower() for x in tag_group]\n",
    "  tag_group = list(dict.fromkeys(tag_group))\n",
    "\n",
    "  \n",
    "  lower_case_col = data[[stamp]]\n",
    "  lower_case_col[stamp] =  lower_case_col[stamp].str.lower()\n",
    "  tags_list = lower_case_col.values.tolist()\n",
    "\n",
    "  tags = []\n",
    "  tag_to_number = {}\n",
    "  number_to_taga = {}\n",
    "\n",
    "  for i in range(len(tag_group)): \n",
    "    tag_to_number[tag_group[i]] = i\n",
    "    number_to_taga[i] = tag_group[i]\n",
    "\n",
    "  for i in range(len(tags_list)):\n",
    "    tags.append(tag_to_number[tags_list[i][0]])\n",
    "\n",
    "    # Se realiza el analisis de los grupos\n",
    "  confusion_birch = confusion_matrix(birch_result, tags)[0:number_clusters]\n",
    "\n",
    "  # Se observar como es que estan por categoria\n",
    "  def get_category(confusion_array, label=''):\n",
    "    print('_'*100)\n",
    "    print('\\nCONFUSION DE:', label, 'CON LA VARIABLE CUALITATIVA', stamp)\n",
    "    keys = list(tag_to_number.keys())\n",
    "    for i in range(number_clusters):\n",
    "      print('\\nCLUSTER #', i)\n",
    "      result = list(confusion_array[i])\n",
    "      index = result.index(max(result))\n",
    "      '''\n",
    "      for i in range(len(result)):\n",
    "        print('%s con %d' %(keys[i], result[i]))\n",
    "      '''\n",
    "\n",
    "      print('Se asegura que es el grupo de: %s con %d' %(keys[index], result[index]))\n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "  get_category(confusion_birch, 'BIRCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2e82e792ab2fe1f79a1681212c8a0df90a82b1b30c69f8528b93f57968011e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
