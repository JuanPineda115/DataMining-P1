{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidad del Valle de Guatemala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto: Defunciones Fetales / Nacimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laura Tamath\t19365 |     Andrea Amaya 19357 |\n",
    "Brandon Hernández\t 19376 |\t\tMartin Amado\t19020 |   Juan Pablo Pineda 19087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "import pyclustertend \n",
    "import random\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, confusion_matrix\n",
    "import sklearn.preprocessing\n",
    "from sklearn.cluster import Birch, KMeans\n",
    "# synthetic classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['dep_reg', 'mun_reg', 'mon_reg', 'year_reg', 'dep_occu','mun_occu','area_geog','sex_death','day_occu',\n",
    "'month_occu','year_occu','part_type','birth_class','via_part','weeks_ges', 'mother_age' ,'mom_country_res','mom_dep_res',\n",
    "'mom_mun_resi', 'mom_group', 'mom_civil_status', 'mom_nationality', 'mom_scholarship', 'mom_occupation', 'cause_death',\n",
    "'assistance_received', 'site_occu', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "\n",
    "quan_vars = ['mother_age', 'total_children', 'total_dead_children', 'total_living_children', 'weeks_ges']\n",
    "qual_vars = []\n",
    "for var in var_names: \n",
    "  if var not in quan_vars: qual_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(data_frame, to_remove, is_quali=True):\n",
    "  df = data_frame.copy()\n",
    "  all_vars = var_names[:]\n",
    "  remove_vars = to_remove[:]\n",
    "\n",
    "  for var in remove_vars: all_vars.remove(var)\n",
    "  df.columns = all_vars\n",
    "  for var in remove_vars:\n",
    "    df[var] = np.full(len(df.index), np.nan if is_quali else 0)\n",
    "  return df.reindex(sorted(df.columns), axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2009 = pd.read_spss('./data/db_2009.sav')\n",
    "df_2010 = pd.read_spss('./data/db_2010.sav')\n",
    "df_2011 = pd.read_spss('./data/db_2011.sav')\n",
    "df_2012 = pd.read_spss('./data/db_2012.sav')\n",
    "df_2013 = pd.read_spss('./data/db_2013.sav')\n",
    "df_2014 = pd.read_spss('./data/db_2014.sav')\n",
    "df_2015 = pd.read_spss('./data/db_2015.sav')\n",
    "df_2016 = pd.read_spss('./data/db_2016.sav')\n",
    "df_2017 = pd.read_spss('./data/db_2017.sav')\n",
    "df_2018 = pd.read_spss('./data/db_2018.sav')\n",
    "df_2019 = pd.read_spss('./data/db_2019.sav')\n",
    "df_2020 = pd.read_spss('./data/db_2020.sav')\n",
    "\n",
    "# Filter data\n",
    "remove_2009 = ['via_part', 'mom_country_res', 'mom_scholarship']\n",
    "remove_2010_2011 = ['mom_country_res']\n",
    "remove_2012_2013_2014 = ['year_occu']\n",
    "remove_2018_2019_2020 = ['area_geog']\n",
    "\n",
    "df_2009 = filter_df(df_2009, remove_2009)\n",
    "df_2009['year_occu'] = np.full(len(df_2009.index), '2009')\n",
    "df_2009['year_reg'] = np.full(len(df_2009.index), '2009')\n",
    "df_2010 = filter_df(df_2010, remove_2010_2011)\n",
    "df_2011 = filter_df(df_2011, remove_2010_2011)\n",
    "df_2012 = filter_df(df_2012, remove_2012_2013_2014)\n",
    "df_2012['year_occu'] = np.full(len(df_2012.index), '2012')\n",
    "df_2013 = filter_df(df_2013, remove_2012_2013_2014)\n",
    "df_2013['year_occu'] = np.full(len(df_2013.index), '2013')\n",
    "df_2014 = filter_df(df_2014, remove_2012_2013_2014)\n",
    "df_2014['year_occu'] = np.full(len(df_2014.index), '2014')\n",
    "df_2015= filter_df(df_2015, [])\n",
    "df_2016= filter_df(df_2016, [])\n",
    "df_2017= filter_df(df_2017, [])\n",
    "df_2018 = filter_df(df_2018, remove_2018_2019_2020)\n",
    "df_2019 = filter_df(df_2019, remove_2018_2019_2020)\n",
    "df_2020 = filter_df(df_2020, remove_2018_2019_2020)\n",
    "data = pd.concat([df_2009, df_2010, df_2011, df_2012, df_2013, df_2014, df_2015, df_2016, df_2017, df_2018, df_2019, df_2020])\n",
    "\n",
    "data['day_occu'] = data['day_occu'].astype(float)\n",
    "data['day_occu'] = data['day_occu'].astype(int)\n",
    "data['year_reg'] = data['year_reg'].astype(float)\n",
    "data['year_reg'] = data['year_reg'].astype(int)\n",
    "data['year_occu'] = data['year_occu'].astype(int)\n",
    "data[qual_vars] = data[qual_vars].astype(str)\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_df = data[quan_vars].replace('Ignorado', -1).fillna(-1)\n",
    "data = pd.concat([quan_df, data[qual_vars]], axis=1)\n",
    "data = data.drop(data[data[quan_vars[0]] < 0].index)\n",
    "data = data.drop(data[data[quan_vars[1]] < 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data.copy()\n",
    "response = ['mom_scholarship', 'mom_dep_res', 'mom_occupation', 'mother_age', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "to_remove = ['Ignorado', 'nan', '99.0']\n",
    "\n",
    "# adding strange\n",
    "for element in list(temp[response[2]].value_counts().index):\n",
    "  try:\n",
    "    float(element)\n",
    "    to_remove.append(element)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "temp = temp[~temp[response[0]].isin(to_remove)]\n",
    "temp = temp.replace(['Ninguno', 'Básico', 'Oficios domésticos no remunerado','Casado','Soltero','Centro de salud','Hospital privado','Hospital público','Seguro social'], ['Ninguna', 'Básica','Oficios domésticos no remunerados','Casada','Soltera','Centro de Salud','Hospital Privado','Hospital Público','Seguro Social'])\n",
    "temp = temp[~temp[response[1]].isin(to_remove)]\n",
    "temp = temp[~temp[response[2]].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp[~temp['area_geog'].isin(to_remove)]\n",
    "temp = temp[~temp['sex_death'].isin(to_remove)]\n",
    "temp = temp[~temp['mom_country_res'].isin(to_remove)]\n",
    "temp = temp[~temp['mom_country_res'].isin(to_remove)]\n",
    "temp = temp.replace(['Eutocio', 'Distocio', 'Unido'], ['Eutócico', 'Distócico', 'Unida'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = temp.sample(n=380, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in qual_vars:\n",
    "    \n",
    "    if (var == \"mun_occu\" or var ==\"mun_reg\" or var == 'mom_mun_resi'):\n",
    "        data.sort_values(by=var, ascending=False)\n",
    "        data[var].head(30).value_counts().plot(kind='bar')\n",
    "    elif (var == 'cause_death'):\n",
    "        data.sort_values(by=var, ascending=False)\n",
    "        data[var].head(10).value_counts().plot(kind='bar')\n",
    "    else:\n",
    "        data[var].value_counts().plot(kind='bar')\n",
    "    plt.figure(figsize=(20,5))\n",
    "    print('\\n'+ var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of place of residence and fetal deaths\n",
    "pd.crosstab(index=data['mom_dep_res'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of residence and fetal deaths\n",
    "pd.crosstab(index=data['mom_dep_res'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of place of birth and fetal deaths\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of birth and fetal deaths\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of type of birth and fetal deaths\n",
    "pd.crosstab(index=data['part_type'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of type of birth and fetal deaths\n",
    "pd.crosstab(index=data['part_type'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mother's marital status and fetal deaths\n",
    "pd.crosstab(index=data['mom_civil_status'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of mother's marital status and fetal deaths\n",
    "pd.crosstab(index=data['mom_civil_status'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of gestation weeks and fetal deaths\n",
    "pd.crosstab(index=data['weeks_ges'],\n",
    "            columns=data['total_dead_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph  of gestation weeks and fetal deaths\n",
    "pd.crosstab(index=data['weeks_ges'],\n",
    "            columns=data['total_dead_children'], margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of place of birth and total born\n",
    "pd.crosstab(index=data['site_occu'],\n",
    "            columns=data['total_children'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean of quant vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_df = data[quan_vars]\n",
    "quan_df[quan_vars] = quan_df[quan_vars].astype(float)\n",
    "quan_df[quan_vars] = quan_df[quan_vars].astype(int)\n",
    "\n",
    "\n",
    "graph_labels = [quan_vars[0], quan_vars[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in quan_vars:\n",
    "  serie = quan_df[quan_df[var] > 0][var]\n",
    "  display(serie.describe())\n",
    "  sns.displot(quan_df[var], kde=True)\n",
    "  print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(serie), 'Skewness:', stats.skew(serie), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = quan_df.corr()\n",
    "cm = np.corrcoef(corrmat.values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=quan_vars, xticklabels=quan_vars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(quan_df, height= 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NACIMIENTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_var_names = ['dep_reg', 'mun_reg', 'mon_reg', 'year_reg', 'inscirption', 'dep_occu','mun_occu', 'area_geog', 'pounds_weight', 'ounces_weight',\n",
    "'day_occu', 'month_occu','year_occu', 'genre', 'part_type','via_part', 'dad_age', 'dad_contry', 'dep_dad', 'mun_dad',\n",
    "'dad_group', 'dad_civil', 'birth_country_dad', 'birt_dep_dad', 'birth_mun_dad', 'dad_nan', 'dad_scholar', 'dad_occup', 'mother_age', \n",
    "'mom_country_res', 'mom_dep_res', 'mom_mun_resi', 'mom_group', 'mom_civil_status', 'birth_country_mom', 'birth_dep_mom', 'birth_mun_mom',\n",
    "'mom_nationality', 'mom_scholarship', 'mom_occupation', 'assistance_received', 'site_occu', 'total_children', 'total_dead_children', \n",
    "'total_living_children']\n",
    "\n",
    "filter = ['dep_reg', 'mun_reg', 'mon_reg', 'year_reg', 'dep_occu', 'mun_occu', 'day_occu', 'month_occu', 'part_type', 'via_part',\n",
    "'mother_age', 'mom_country_res', 'mom_dep_res', 'mom_mun_resi', 'mom_group', 'mom_civil_status', 'mom_scholarship', 'mom_occupation', \n",
    "'assistance_received', 'site_occu', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "\n",
    "nc_quan_vars = ['mother_age', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "nc_qual_vars = []\n",
    "for var in filter: \n",
    "  if var not in nc_quan_vars: nc_qual_vars.append(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_filter_df(data_frame, to_remove, is_quali=True):\n",
    "  df = data_frame.copy()\n",
    "  all_vars = nc_var_names[:]\n",
    "  remove_vars = to_remove[:]\n",
    "\n",
    "  for var in remove_vars: all_vars.remove(var)\n",
    "  df.columns = all_vars\n",
    "  for var in remove_vars:\n",
    "    df[var] = np.full(len(df.index), np.nan if is_quali else 0)\n",
    "  return df.reindex(sorted(df.columns), axis=1).copy()\n",
    "\n",
    "def drop_df_colums(data_frame, to_remove):\n",
    "  df = data_frame.copy()\n",
    "  all_vars = nc_var_names[:]\n",
    "  remove_vars = to_remove[:]\n",
    "\n",
    "  for var in remove_vars: all_vars.remove(var)\n",
    "  for var in all_vars:\n",
    "    df = df.drop(var, axis=1)\n",
    "  return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_2009 = pd.read_spss('./data/nc_2009.sav')\n",
    "nc_2010 = pd.read_spss('./data/nc_2010.sav')\n",
    "nc_2011 = pd.read_spss('./data/nc_2011.sav')\n",
    "nc_2012 = pd.read_spss('./data/nc_2012.sav')\n",
    "nc_2013 = pd.read_spss('./data/nc_2013.sav')\n",
    "nc_2014 = pd.read_spss('./data/nc_2014.sav')\n",
    "nc_2015 = pd.read_spss('./data/nc_2015.sav')\n",
    "nc_2016 = pd.read_spss('./data/nc_2016.sav')\n",
    "nc_2017 = pd.read_spss('./data/nc_2017.sav')\n",
    "nc_2018 = pd.read_spss('./data/nc_2018.sav')\n",
    "nc_2019 = pd.read_spss('./data/nc_2019.sav')\n",
    "nc_2020 = pd.read_spss('./data/nc_2020.sav')\n",
    "\n",
    "nc_2009 = nc_filter_df(nc_2009, ['inscirption', 'via_part', 'dad_contry', 'birth_country_dad', 'dad_scholar', 'mom_country_res', 'birth_country_mom', 'mom_scholarship'])\n",
    "nc_2009['year_reg'] = np.full(len(nc_2009.index), '2009')\n",
    "nc_2010 = nc_filter_df(nc_2010, ['inscirption', 'via_part', 'dad_contry', 'birth_country_dad', 'mom_country_res', 'birth_country_mom'])\n",
    "nc_2010['year_reg'] = np.full(len(nc_2010.index), '2010')\n",
    "nc_2011 = nc_filter_df(nc_2011, ['inscirption', 'via_part', 'dad_contry', 'birth_country_dad', 'mom_country_res', 'birth_country_mom'])\n",
    "nc_2011['year_reg'] = np.full(len(nc_2011.index), '2011')\n",
    "nc_2012 = nc_filter_df(nc_2012, ['inscirption', 'area_geog', 'year_occu', 'via_part'])\n",
    "nc_2012['year_reg'] = np.full(len(nc_2012.index), '2012')\n",
    "nc_2013 = nc_filter_df(nc_2013, ['inscirption', 'area_geog', 'year_occu', 'via_part'])\n",
    "nc_2013['year_reg'] = np.full(len(nc_2013.index), '2013')\n",
    "nc_2014 = nc_filter_df(nc_2014, ['inscirption', 'area_geog',  'year_occu', 'via_part', 'dad_nan', 'mom_nationality'])\n",
    "nc_2014['year_reg'] = np.full(len(nc_2014.index), '2014')\n",
    "nc_2015 = nc_filter_df(nc_2015, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2015['year_reg'] = np.full(len(nc_2015.index), '2015')\n",
    "nc_2016 = nc_filter_df(nc_2016, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2016['year_reg'] = np.full(len(nc_2016.index), '2016')\n",
    "nc_2017 = nc_filter_df(nc_2017, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2017['year_reg'] = np.full(len(nc_2017.index), '2017')\n",
    "nc_2018 = nc_filter_df(nc_2018, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2018['year_reg'] = np.full(len(nc_2018.index), '2018')\n",
    "nc_2019 = nc_filter_df(nc_2019, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2019['year_reg'] = np.full(len(nc_2019.index), '2019')\n",
    "nc_2020 = nc_filter_df(nc_2020, ['area_geog', 'dad_nan', 'mom_nationality'])\n",
    "nc_2020['year_reg'] = np.full(len(nc_2020.index), '2020')\n",
    "\n",
    "nc_2009 = drop_df_colums(nc_2009, filter)\n",
    "nc_2010 = drop_df_colums(nc_2010, filter)\n",
    "nc_2011 = drop_df_colums(nc_2011, filter)\n",
    "nc_2012 = drop_df_colums(nc_2012, filter)\n",
    "nc_2013 = drop_df_colums(nc_2013, filter)\n",
    "nc_2014 = drop_df_colums(nc_2014, filter)\n",
    "nc_2015 = drop_df_colums(nc_2015, filter)\n",
    "nc_2016 = drop_df_colums(nc_2016, filter)\n",
    "nc_2017 = drop_df_colums(nc_2017, filter)\n",
    "nc_2018 = drop_df_colums(nc_2018, filter)\n",
    "nc_2019 = drop_df_colums(nc_2019, filter)\n",
    "nc_2020 = drop_df_colums(nc_2020, filter)\n",
    "\n",
    "nc_data = pd.concat([nc_2009, nc_2010, nc_2011, nc_2012, nc_2013, nc_2014, nc_2015, nc_2016, nc_2017, nc_2018, nc_2019, nc_2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_nc = nc_data[nc_quan_vars].replace('Ignorado', -1).fillna(-1)\n",
    "nc_data = pd.concat([quan_nc, nc_data[nc_qual_vars]], axis=1)\n",
    "nc_data = nc_data.drop(nc_data[nc_data[quan_vars[0]] < 0].index)\n",
    "nc_data = nc_data.drop(nc_data[nc_data[quan_vars[1]] < 0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nc = nc_data.copy()\n",
    "response = ['mom_scholarship', 'mom_dep_res', 'mom_occupation', 'mother_age', 'total_children', 'total_dead_children', 'total_living_children']\n",
    "to_remove = ['Ignorado', 'nan', '99.0']\n",
    "\n",
    "# adding strange\n",
    "for element in list(temp_nc[response[2]].value_counts().index):\n",
    "  try:\n",
    "    float(element)\n",
    "    to_remove.append(element)\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "temp_nc = temp_nc[~temp_nc[response[0]].isin(to_remove)]\n",
    "temp_nc = temp_nc.replace(['Ninguno', 'Básico', 'Oficios domésticos no remunerado'], ['Ninguna', 'Básica','Oficios domésticos no remunerados'])\n",
    "temp_nc = temp_nc[~temp_nc[response[1]].isin(to_remove)]\n",
    "temp_nc = temp_nc[~temp_nc[response[2]].isin(to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nc = temp_nc.replace(['Médica', 'Médico', 'Medico', 'Medica'], 'Medico')\n",
    "temp_nc = temp_nc.replace(['Paramédico', 'Paramédica'], 'Paramedico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_data = temp_nc.sample(n=380, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_quan_df = nc_data[nc_quan_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in nc_quan_vars:\n",
    "  serie = nc_quan_df[nc_quan_df[var] > 0][var]\n",
    "  display(serie.describe())\n",
    "  sns.displot(nc_quan_df[var], kde=True)\n",
    "  print('\\033[1m' + var + '\\033[0m' + ': Kurtosis:', stats.kurtosis(serie), 'Skewness:', stats.skew(serie), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 #number of variables for heatmap\n",
    "corrmat = nc_quan_df.corr()\n",
    "cm = np.corrcoef(corrmat.values.T)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=nc_quan_vars, xticklabels=nc_quan_vars)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(nc_quan_df, height= 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in nc_qual_vars:\n",
    "    if (var == \"mun_reg\" or var == 'mun_occu' or var == 'mom_mun_resi'):\n",
    "        nc_data[var].head(30).value_counts().plot(kind='bar')\n",
    "    else:\n",
    "        nc_data[var].value_counts().plot(kind='bar')\n",
    "\n",
    "    plt.figure(figsize=(20,5))\n",
    "    print('\\n'+ var)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom departament residence and births\n",
    "pd.crosstab(index=nc_data['mom_dep_res'],\n",
    "            columns=nc_data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of mom departament residence and births\n",
    "pd.crosstab(index=nc_data['mom_dep_res'],\n",
    "            columns=nc_data['total_living_children'], dropna=True, margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom scholarship and births\n",
    "pd.crosstab(index=nc_data['mom_scholarship'],\n",
    "            columns=nc_data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of mom scholarship residence and births\n",
    "pd.crosstab(index=nc_data['mom_dep_res'],\n",
    "            columns=nc_data['total_living_children'], dropna=True, margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of mom occupation and births\n",
    "pd.crosstab(index=nc_data['mom_scholarship'],\n",
    "            columns=nc_data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of mom occupation residence and births\n",
    "pd.crosstab(index=nc_data['mom_scholarship'],\n",
    "            columns=nc_data['total_living_children'], dropna=True, margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table of site of occurrence and births\n",
    "pd.crosstab(index=nc_data['site_occu'],\n",
    "            columns=nc_data['total_living_children'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph of place of site of occurrence e and births\n",
    "pd.crosstab(index=nc_data['site_occu'],\n",
    "            columns=nc_data['total_living_children'], dropna=True, margins=True).apply(lambda r: r/r.sum() *100, axis=1).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Comience  describiendo  cuantas  variables  y  observaciones  tiene  disponibles,  el tipo de cada una de las variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Haga un resumen de las variables numéricas e investigue si siguen una distribución normal y tablas de frecuencia para las variables categóricas, escriba lo que vaya encontrando. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cruce  las  variables  que  considere  que  son  las  más  importantes  para  hallar  los elementos  clave  que  lo  pueden  llevar  a  comprender  lo  que  está  causando  el problema encontrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Haga gráficos exploratorios que le de ideas del estado de los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Haga un agrupamiento (clustering) e interprete los resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "X_scale=sklearn.preprocessing.scale(quan_df)\n",
    "\n",
    "pyclustertend.hopkins(X_scale, len(X_scale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de hopkins es de 0.11, por lo que vale la pena hacer el agrupamiento al tener datos distribuidos de manera uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyclustertend.vat(X_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la gráfica de codo para encontrar la cantidad óptima de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeroClusters = range(1,11)\n",
    "\n",
    "wcss = []\n",
    "# Obtenemos 10 posibles clusters\n",
    "for i in numeroClusters:\n",
    "    # Se calcula la kmean con esa cantidad de clusters\n",
    "    kmeans = cluster.KMeans(n_clusters=i)\n",
    "    kmeans.fit(X_scale)\n",
    "    # Obtenemos la inercia\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Graficando\n",
    "plt.plot(numeroClusters, wcss)\n",
    "plt.xlabel(\"Cantidad de clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.title(\"Gráfico de Codo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se harán uso de 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = 4\n",
    "colors = ['mediumslateblue', 'skyblue', 'pink', 'cornflowerblue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo\n",
    "birch_model = Birch(threshold=1.5, n_clusters=number_clusters)\n",
    "birch_model.fit(X_scale)\n",
    "\n",
    "# Obtenemos los puntos y los clusters\n",
    "birch_result = birch_model.predict(X_scale)\n",
    "\n",
    "for i in range(number_clusters):\n",
    "    # Graficar los clusters\n",
    "    plt.scatter(X_scale[birch_result == i, 0], X_scale[birch_result == i, 1], s = 100, c = colors[i], label = \"Cluster %d\" %i)\n",
    "\n",
    "plt.title(\"Metodo de BIRCH\")\n",
    "plt.xlabel(graph_labels[0])\n",
    "plt.ylabel(graph_labels[1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_silhouette(clusterer, n_clusters, label):\n",
    "    fig, ax = plt.subplots(figsize=(1,1))\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    ax.set_xlim([-0.1, 1])\n",
    "    ax.set_ylim([0, len(X_scale) + (n_clusters + 1) * 10])\n",
    "\n",
    "    cluster_labels = clusterer.fit_predict(X_scale)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X_scale, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score of\",\n",
    "        label,\n",
    "        'is:',\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    sample_silhouette_values = silhouette_samples(quan_df, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = colors[i]\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_yticks([]) \n",
    "    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "make_silhouette(birch_model, number_clusters, 'BIRCH')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stamps = ['dep_reg', 'assistance_received', 'site_occu', 'cause_death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiando las variables cualitativas a numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stamp in stamps:\n",
    "  aux = data.groupby(by=stamp)\n",
    "  \n",
    "  \n",
    "  tag_group = list(aux.groups.keys())\n",
    "  tag_group = [x.lower() for x in tag_group]\n",
    "  tag_group = list(dict.fromkeys(tag_group))\n",
    "\n",
    "  \n",
    "  lower_case_col = data[[stamp]]\n",
    "  lower_case_col[stamp] =  lower_case_col[stamp].str.lower()\n",
    "  tags_list = lower_case_col.values.tolist()\n",
    "\n",
    "  tags = []\n",
    "  tag_to_number = {}\n",
    "  number_to_taga = {}\n",
    "\n",
    "  for i in range(len(tag_group)): \n",
    "    tag_to_number[tag_group[i]] = i\n",
    "    number_to_taga[i] = tag_group[i]\n",
    "\n",
    "  for i in range(len(tags_list)):\n",
    "    tags.append(tag_to_number[tags_list[i][0]])\n",
    "\n",
    "    # Se realiza el analisis de los grupos\n",
    "  confusion_birch = confusion_matrix(birch_result, tags)[0:number_clusters]\n",
    "\n",
    "  # Se observar como es que estan por categoria\n",
    "  def get_category(confusion_array, label=''):\n",
    "    print('_'*100)\n",
    "    print('\\nCONFUSION DE:', label, 'CON LA VARIABLE CUALITATIVA', stamp)\n",
    "    keys = list(tag_to_number.keys())\n",
    "    for i in range(number_clusters):\n",
    "      print('\\nCLUSTER #', i)\n",
    "      result = list(confusion_array[i])\n",
    "      index = result.index(max(result))\n",
    "      '''\n",
    "      for i in range(len(result)):\n",
    "        print('%s con %d' %(keys[i], result[i]))\n",
    "      '''\n",
    "\n",
    "      print('Se asegura que es el grupo de: %s con %d' %(keys[index], result[index]))\n",
    "    \n",
    "    print('_'*100)\n",
    "\n",
    "  get_category(confusion_birch, 'BIRCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALISIS PARTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showGraph(): \n",
    "    scaler.fit(X_test)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    pca2 = PCA(n_components=2)\n",
    "    X_test_scaled_reduced = pca2.fit_transform(X_test_scaled)\n",
    "\n",
    "    svm_model = SVC(kernel=grid.best_params_['SupVM__kernel'], C=float(grid.best_params_['SupVM__C']), gamma=float(grid.best_params_['SupVM__gamma']))\n",
    "\n",
    "    classify = svm_model.fit(X_test_scaled_reduced, y_test)\n",
    "\n",
    "    def plot_contours(ax, clf, xx, yy, **params):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        print ('initial decision function shape; ', np.shape(Z))\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        print ('after reshape: ', np.shape(Z))\n",
    "        out = ax.contourf(xx, yy, Z, **params)\n",
    "        return out\n",
    "\n",
    "    def make_meshgrid(x, y, h=.1):\n",
    "        x_min, x_max = x.min() - 1, x.max() + 1\n",
    "        y_min, y_max = y.min() - 1, y.max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                            np.arange(y_min, y_max, h))#,\n",
    "                            #np.arange(z_min, z_max, h))\n",
    "        return xx, yy\n",
    "\n",
    "    X0, X1 = X_test_scaled_reduced[:, 0], X_test_scaled_reduced[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    cdict1={0:'lime',1:'deeppink'}\n",
    "\n",
    "    Y_tar_list = y_test.tolist()\n",
    "    yl1= [int(target1) for target1 in Y_tar_list]\n",
    "    labels1=yl1\n",
    "    \n",
    "    labl1={0:'Defunciones',1:'Nacimientos'}\n",
    "    marker1={0:'*',1:'d'}\n",
    "    alpha1={0:.8, 1:0.5}\n",
    "\n",
    "    for l1 in np.unique(labels1):\n",
    "        ix1=np.where(labels1==l1)\n",
    "        ax.scatter(X0[ix1],X1[ix1], c=cdict1[l1],label=labl1[l1],s=70,marker=marker1[l1],alpha=alpha1[l1])\n",
    "\n",
    "    ax.scatter(svm_model.support_vectors_[:, 0], svm_model.support_vectors_[:, 1], s=40, facecolors='none', \n",
    "            edgecolors='navy', label='Support Vectors')\n",
    "\n",
    "    plot_contours(ax, classify, xx, yy,cmap='seismic', alpha=0.4)\n",
    "    plt.legend(fontsize=15)\n",
    "\n",
    "    plt.xlabel(\"1st Principal Component\",fontsize=14)\n",
    "    plt.ylabel(\"2nd Principal Component\",fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniendo defunciones y nacimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"response\"] = np.full(380, 0) # 0 defuncion\n",
    "nc_data[\"response\"] = np.full(380, 1) # 1 nacimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_joined = pd.concat([data, nc_data])\n",
    "data_joined = data_joined.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_joined = data_joined.replace(\"Ignorado\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_quan_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_data = data_joined.copy()\n",
    "target = copied_data.pop(\"response\")\n",
    "\n",
    "\n",
    "quant_data = copied_data[nc_quan_vars]\n",
    "\n",
    "# 70% de entrenamiento y 30% prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(quant_data, target,test_size=0.3,train_size=0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(quant_data)\n",
    "feature_scaled = scaler.transform(quant_data)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(feature_scaled)\n",
    "feature_scaled_pca = pca.transform(feature_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_steps = [('scaler', StandardScaler()), ('pca', PCA()), ('SupVM', SVC())]\n",
    "\n",
    "check_params= {\n",
    "    'pca__n_components': [2], \n",
    "    'SupVM__C': [0.1, 0.5, 1], \n",
    "    'SupVM__gamma' : [0.01, 0.5, 1],\n",
    "    'SupVM__kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "\n",
    "grid = GridSearchCV(pipeline,check_params,refit=True,verbose=2)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"grid best params: \", grid.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "# print (Y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "print (\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print (\"Precision:\", precision_score(y_test, y_pred,average='weighted') )\n",
    "print (\"Recall: \", recall_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(y_pred)\n",
    "print('Nacimientos:', pred.count(1))\n",
    "print('Defunciones:', pred.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm = list(cm)\n",
    "labels = ['Defunciones', 'Nacimientos']\n",
    "\n",
    "print('            '+labels[0]+'  '+labels[1])\n",
    "print(labels[0], cm[0][0], '        ',cm[0][1])\n",
    "print(labels[1], cm[1][0], '         ',cm[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con variable cualitativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data and encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_qual_vars = response[0:3]\n",
    "nc_qual_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 CATEGORIAS\n",
    "* categoria1 alta movilidad\n",
    "* categoria2 movilidad normal\n",
    "* categoria3 poca movilidad\n",
    "* categoria4 no especificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_joined = data_joined.replace(\"No especificado en otro grupo\", \"categoria4\")\n",
    "data_joined = data_joined.replace(\"NEOG\", \"categoria4\")\n",
    "data_joined = data_joined.replace(\"Ocupacion no bien especificada\", \"categoria4\")\n",
    "data_joined = data_joined.replace(\"Ocupación no bien especificada\", \"categoria4\")\n",
    "\n",
    "data_joined = data_joined.replace(\"Oficinistas\", \"categoria3\")\n",
    "data_joined = data_joined.replace(\"Operadores de instalaciones fijas y máquinas\", \"categoria3\")\n",
    "data_joined = data_joined.replace(\"Vendedores\", \"categoria3\")\n",
    "data_joined = data_joined.replace(\"Empleados contables y encargados del registro de materiales\", \"categoria3\")\n",
    "data_joined = data_joined.replace(\"Secretarios (general)\", \"categoria3\")\n",
    "data_joined = data_joined.replace(\"Trabajadores de los servicios personales\", \"categoria3\")\n",
    "data_joined = data_joined.replace(\"Empleados de contabilidad y cálculo de costos\", \"categoria3\")\n",
    "\n",
    "data_joined = data_joined.replace(\"Profesionales de la enseñanza\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Estudiante\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Maestros de enseñanza primaria\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Demostradores de tiendas\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Otro personal de apoyo administrativo\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Operarios y oficiales de procesamiento de alimentos, de la confección, ebanistas, otros artesanos y afines\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Trabajadores y asistentes sociales de nivel medio\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Directores administradores y comerciales\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Profesionales en derecho, en ciencias sociales y culturales\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Profesionales de nivel medio de servicios jurídicos, sociales, culturales y afines\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Profesionales de las ciencias y de la ingeniería\", \"categoria2\")\n",
    "data_joined = data_joined.replace(\"Trabajadores de los cuidados personales\", \"categoria2\")\n",
    "\n",
    "\n",
    "data_joined = data_joined.replace(\"Oficios domésticos no remunerados\", \"categoria1\")\n",
    "data_joined = data_joined.replace(\"Comerciantes de tiendas\", \"categoria1\")\n",
    "data_joined = data_joined.replace(\"Profesionales de nivel medio de la salud\", \"categoria1\")\n",
    "data_joined = data_joined.replace(\"Maestros preescolares\", \"categoria1\")\n",
    "data_joined = data_joined.replace(\"Profesionales de nivel medio de enfermería\", \"categoria1\")\n",
    "data_joined = data_joined.replace(\"Profesionales de la salud\", \"categoria1\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se preparan los preprocesadores\n",
    "categorical_preprocessor = LabelEncoder()\n",
    "X_2 = data_joined[nc_qual_vars].apply(categorical_preprocessor.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_data = data_joined.copy()\n",
    "target = copied_data.pop(\"response\")\n",
    "\n",
    "\n",
    "quant_data = copied_data[nc_quan_vars]\n",
    "quant_data =  pd.concat([quant_data, X_2], axis=1)\n",
    "\n",
    "\n",
    "# 70% de entrenamiento y 30% prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(quant_data, target,test_size=0.3,train_size=0.7, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(quant_data)\n",
    "feature_scaled = scaler.transform(quant_data)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(feature_scaled)\n",
    "feature_scaled_pca = pca.transform(feature_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_steps = [('scaler', StandardScaler()), ('pca', PCA()), ('SupVM', SVC())]\n",
    "\n",
    "check_params= {\n",
    "    'pca__n_components': [2], \n",
    "    'SupVM__C': [0.1, 0.5, 1], \n",
    "    'SupVM__gamma' : [0.01, 0.5, 1],\n",
    "    'SupVM__kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(pipe_steps)\n",
    "\n",
    "grid = GridSearchCV(pipeline,check_params,refit=True,verbose=2)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"grid best params: \", grid.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "# print (Y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix: \\n\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "print (\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print (\"Precision:\", precision_score(y_test, y_pred,average='weighted') )\n",
    "print (\"Recall: \", recall_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm = list(cm)\n",
    "labels = ['Defunciones', 'Nacimientos']\n",
    "\n",
    "print('            '+labels[0]+'  '+labels[1])\n",
    "print(labels[0], cm[0][0], '        ',cm[0][1])\n",
    "print(labels[1], cm[1][0], '         ',cm[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showGraph()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2e82e792ab2fe1f79a1681212c8a0df90a82b1b30c69f8528b93f57968011e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
